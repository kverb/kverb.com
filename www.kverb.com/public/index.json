[{"content":"Finally, I\u0026rsquo;ve found a relatively simple and functional GUI e-mail client for Linux - Geary.\nAlthough it is built for the Gnome desktop environment, it works fine in SwayWM, runs in wayland, even with some nice transition animations. It also (and i\u0026rsquo;m totally guessing here) seems to obey system-wide GTK theme configuration. I\u0026rsquo;m guessing because I never actually configured this myself; Garuda Linux on Sway seems to have shipped with a nord-esque theme out of the box.\nIt has a basic set of GUI email client features, but the most important for me is \u0026ldquo;single-key keyboard shortcuts\u0026rdquo;, similar to how one might use mutt or the Gmail web interface. It\u0026rsquo;s unfortunate how few email clients support this, but the vi-like navigation, including \u0026rsquo;e\u0026rsquo; for archive has become so ingrained to my muscle memory that anything that ships without this is a deal-breaker for me.\nAnother nice feature is jamming \u0026lsquo;j\u0026rsquo; and \u0026lsquo;k\u0026rsquo; to navigate through emails is FAST.\nProton Mail Bridge integration # I\u0026rsquo;ve been using Proton Mail with a custom domain for a few years, and while the mobile app is \u0026hellip; fine, the webmail has never worked well for me, even with the \u0026ldquo;new and improved\u0026rdquo; flavor. Interestingly, Proton mail doesn\u0026rsquo;t support IMAP directly. You must first run a local \u0026ldquo;bridge\u0026rdquo; daemon client, which presumably handles the encrypt/decrypt transport to proton servers, then makes your email locally available via IMAP. It seems all a bit convoluted at first, but is overall quite a brilliant solution. And I was pleasantly surprised at easy the bridge app was to use and configure with Geary. Similarly to Gmail \u0026ldquo;application passwords\u0026rdquo;, you can generate a new app-specific password for any of the mail aliases you have in proton (i think you have to enable the \u0026ldquo;splt addresses\u0026rdquo; feature in the account settings). In Geary, point your email account\u0026rsquo;s IMAP server to 127.0.0.1 and the correct port, and voila. An added benefit, for me, is that because I use a \u0026ldquo;blackhole@\u0026rdquo; account, and allow @ to forward into that account, I can now have a separate inbox for all those ham emails. Anytime I have to sign up for a service, I\u0026rsquo;ll usually use a specific email alias for that service, and it will end up in the blackhole@ inbox. This is great because it typically just drops all the crappy marketing emails and newsletters there, while keeping my personal ken@ inbox cleaner.\nNOTE: On proton mobile, it all just ends up in one inbox. Flaws but not deal-breakers # Possibly a sway limitation, but I can\u0026rsquo;t seem to be able to increase the font size in the message view list. it\u0026rsquo;s simple on features, and i probably wouldn\u0026rsquo;t sync a lifetime of mail to my local system, which limits the search capabilities. HTML formatting is fine, but feel different from the gmail webmail interface. There\u0026rsquo;s no shortcut to navigate between messages in a conversation. It seems like you have to use and . It\u0026rsquo;s a bit clunky. ","date":"11 June 2022","permalink":"/posts/geary/","section":"Posts","summary":"A short review of the Geary E-mail client on Linux","title":"Geary"},{"content":"Created: March 10, 2022 1:59 PM Last Edited Time: March 10, 2022 6:28 PM\nmeta # Grafana - is the front-end dashboard website, that supports querying datasources, and displaying visualizatoins of the data. It is basically a full-stack webapp that you can self-host.\nPrometheus - is a metrics collection endpoint. It runs as an http server that also “scrapes” target services, periodically via http. It becomes the primary data source for counters/timeseries when using grafana dashboards. Conceptually, it is the main database for recording and facilitating queries against the metrics.\nNginx Node exporter - it’s a confusing name, but in order for prometheus to scrape any useful data, it needs to be available to scrape on the target. The “node exporter” is what makes metrics data available for prometheus to scrape. There are many node exporters available for popular software services, including nginx.\nLoki - is like prometheus, but for text logs. I.e., it receives a stream of text, generally the recent lines of a log file. The encoding and format of each line can be arbitary. Loki will parse them as-is. Like prometheus, it becomes the primary datasource for log-based data and queries.\nPromtail - is a service that runs on the target node. It is what pushes data into Loki (Unlike prometheus, loki does not operate on a pull/scrape model). You can also think of the target as the “source of the logs data”. In this case, our target/source of data is our nginx metrics and logs. Promtail needs read access to the raw log files, and then streams it to Loki. Because of this, I run promtail directly on the nginx container.\nassumptions / pre-conditions # grafana, prometheus, and loki are already up and running correctly. These can/should be run on a separate container from nginx. nginx is on a tailscale network that can access prometheus and Loki nodes in my setup, loki runs on the prometheus node this isn’t necessary; it just makes resolving the hosts secure and easy You’re not using something like docker containers. If you are, there are plenty of guides out there on how to do that. Set up the node exporter # There are probably distro packages/procedures for installing the nginx node exporter. Here’s how you can manually do it on linux. I do this from a root shell on the nginx container. The overall process is: download the ‣ binary release, unzip and install the executable somewhere, create a systemd service definition.\n# download the binary cd ~/downloads wget [https://github.com/nginxinc/nginx-prometheus-exporter/releases/download/v0.10.0/nginx-prometheus-exporter_0.10.0_linux_amd64.tar.gz](https://github.com/nginxinc/nginx-prometheus-exporter/releases/download/v0.10.0/nginx-prometheus-exporter_0.10.0_linux_amd64.tar.gz) -O ~/downloads/npe.tar.gz # un-archive tar -xvf npe.tar.gz # copy the binary to somewhere sane. Here i\u0026#39;ve chosen /opt/ cp nginx-prometheus-exporter /opt/ We’ll need to make a systemd service definition file. Here’s mine:\n$ cat /etc/systemd/system/nginx-prometheus-exporter.service [Unit] Description=Prometheus Node Exporter for Nginx ConditionFileIsExecutable=/opt/nginx-prometheus-exporter After=syslog.target network-online.target [Service] StartLimitInterval=5 StartLimitBurst=10 ExecStart=/opt/nginx-prometheus-exporter SyslogIdentifier=prometheus WorkingDirectory=/opt/ Restart=always RestartSec=10 [Install] WantedBy=multi-user.target In order for the node exporter to provide useful metrics, we need to enable the stub_status module for nginx. It’s basically a special built-in http endpoint that can be exposed through your nginx conf. This is the relevant snippet, that should go in a sites-enabled or in the root conf:\nserver { server_name nginx; listen 8080; # i\u0026#39;m not sure this port can be changed easily location = /stub_status { stub_status; } } On the prometheus node, enable the scraping target\n$ cat /etc/prometheus/prometheus.yml \u0026lt; ... \u0026gt; - job_name: \u0026#34;nginx_node\u0026#34; static_configs: - targets: [\u0026#34;nginx:9113\u0026#34;] # NOTE: this needs to point to your nginx host Loki + Promtail # As stated in the preface, in order to make the the nginx logs accessible in grafana+loki, we need a promtail instance that can read the raw logs files. On docker-style containers, there’s various incantations to bind-mount the nginx logs directories into a promtail container, but I think it’s just much more straight-forward to install promtail directly in the nginx container. Again, there’s probably a simple and straighforward way to install promtail via your distro’s package manager, but here are the manual steps:\n# download the binary cd ~/downloads wget https://github.com/grafana/loki/releases/download/v2.4.2/promtail-linux-amd64.zip -O ~/promtail.zip # unzip and copy to a sane place unzip promtail.zip mkdir /opt/promtail cp /opt/promtail/promtail Promtail requires a small config file. You can put it anywhere, but I’ll put it in /opt/promtail\n$ cat /opt/promtail/promtail.yml server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: # make sure this host is your LOKI hostname - url: http://prometheus:3100/loki/api/v1/push scrape_configs: - job_name: system static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/*log - job_name: nginx static_configs: - targets: - localhost labels: job: nginx __path__: /var/log/nginx/*log And create a systemd service definition:\n$ cat /etc/systemd/system/promtail.service [Unit] Description=promtail is the agent responsible for gathering logs and sending them to Loki. Documentation=https://grafana.com/docs/loki/latest/clients/promtail/ Requires=network-online.target After=network-online.target [Service] Type=simple ExecStart=/opt/promtail/promtail -positions.file /var/lib/promtail/positions.yml -config.file /opt/promtail/promtail.yml User=promtail TimeoutStopSec=30s NoNewPrivileges=true MemoryDenyWriteExecute=true RestrictRealtime=true ProtectHome=true ProtectSystem=strict ReadWritePaths=/var/lib/promtail PrivateTmp=true PrivateDevices=true ProtectKernelTunables=true ProtectKernelModules=true ProtectControlGroups=true [Install] WantedBy=multi-user.target Reload systemd and enable and start the new services:\n$ systemctl daemon-reload $ systemctl enable --now promtail $ systemctl enable --now nginx-prometheus-exporter You can verify nginx-prometheus-exporter is working by checking its http endpoint: http://nginx:9113/metrics\nAnd promtail: http://nginx:9080/targets\nDashboards in grafana # I’m using dashboard queries and viz based on this dashboard. It requires some slight modifications to the queries to get accurate data. A lot of the queries have extra filters that are specific to the original author.\n","date":"11 June 2022","permalink":"/posts/nginx_monitoring/","section":"Posts","summary":"Created: March 10, 2022 1:59 PM Last Edited Time: March 10, 2022 6:28 PM","title":"nginx metrics and log collection with grafana, prometheus, loki"},{"content":"Very loosely organized notes of server builds.\nPlex server # Ideally 1U or 2U. Probably should be an intel chip that supports \u0026ldquo;Quick Sync\u0026rdquo; for efficient video transcoding. Supermicro barebones seems to have the ideal chassis+mobo combo. Must have 10Gbe or SFP+\n1U barebones/prebiult supermicro # mini, low power SYS-E300-9C with coffee lake i7 can this handle transcodes? lol $5k SYS-110P-FRDN2T 16\u0026quot; depth uses LGA-4189 socket SYS-110C-FHN4T 12\u0026quot; depth uses cheaper CPU LGA-1200 socket 5019c-l doesn\u0026rsquo;t have 10Gbe nic ; can probably be added later DIY # Should probably go with intel 1151 socket, based on serverbuilds.net and Perfect Media Server guy\nFinal candidate build (~$2425) # $129 MSI B560M PRO-VDH WIFI LGA 1200 Intel B560 SATA 6Gb/s Micro ATX Intel Motherboard $215 Intel Core i5-11400 - Core i5 11th Gen Rocket Lake 6-Core 2.6 GHz LGA 1200 65W $69 Thermaltake Engine 27 1U Low-Profile 70W Intel 60mm Low Noise PWM Fan Forty Fan Blade CPU Cooler $272 (2x$136) Crucial 32GB Single DDR4 3200 MT/s CL22 DIMM 288-Pin Memory $99 SAMSUNG 980 M.2 2280 1TB PCI-Express 3.0 x4, NVMe 1.4 V-NAND MLC Internal Solid State Drive $199 PLINKUSA 2U Rackmount (2x5.25+2x3.5 HD+2x2.5 HD)(15.16\u0026quot; Deep) $95 CORSAIR RM650 650 W ATX 80 PLUS GOLD Certified Full Modular Power Supply (already purchased) $1375 ($687x2) SAMSUNG 870 QVO Series 2.5\u0026quot; 8TB SATA III Samsung 4-bit MLC V-NAND Internal Solid State Drive (SSD) sata cables already ordered #TODO: 10Gbe/SFP+ card Potential build: # mobo/cpu opt 1 $249 AsRock Rack E3C246D4U $299 Intel Xeon E-2126G Coffee Lake mobo/cpu opt 2 $129 MSI socket 1200 $175 core i5 10th gen (rocket lake) 65W $99 SAMSUNG 980 M.2 2280 1TB $199 2U PlinkUSA chassi I dont think the Noctua low-profile will fit in 1U $280 4x16 GB RAM $95 650W PSU NAS # Can probably DIY\nneed to understand SAS vs SATA; physical disc connection limitations\napparently 6 2.5\u0026quot; ssd can fit in 1 5.25\u0026quot; drive bay\nchassis:\nrosewill, plinkusa, chenbro, silverstone seem to be the only short-depth ones stylish 2U 16\u0026quot; deep PLinkUSA 2 x 5.25\u0026quot; front drive bays 2U short PLinkUSA 15\u0026quot; depth uses full ATX PSU, which seems weird 2U Athena power 14\u0026quot; depth has PSU AND 2 x 5.25\u0026quot; front drive bays; could usewith icy dock mobo\nepyc 3000 series? SoC , lots of PCI \u0026ldquo;lanes\u0026rdquo;, fuckton of RAM, and low power. Doesn\u0026rsquo;t typically include 10Gbe Intel Xeon D series SoC, typically includes 10Gbe Gigabyte Xeon D-1500 most seem to use OCUlink or sas breakout cables to connect SATA drives high end SoC: AsRock Rack D2143D4I2-2T Mini-ITX Xeon D-2100 overkill option: ASRock Rome Epyc 7000 series would require a 7000 series CPU; uses too much power AsRock Rack X570D4U-2L2T ryzen zen 3 has dual 10Gbe \u0026amp; 8 SATA ports Premade systems # 1U Supermicro SYS-5018D-FN4T dual 10Gbe 6 SATA ports M Key 2242/2280 might be able to just put SSDs in the empty space inside the chassis, or into an Icy Dock cage 1U ASRock ryzen in a supermicro chassis kinda strange, but has dual 10Gbe \u0026amp; 8 SATA ports. ","date":"11 June 2022","permalink":"/posts/server/","section":"Posts","summary":"Very loosely organized notes of server builds.","title":"server notes"},{"content":"meta # These are some notes gathered on my exploration of transcoding videos for more reliable playback. This doesn’t cover the downloading of the video files; just post-processing them into different formats for playback, primarily on an Apple TV 4k (2019).\nHardware used:\nAMD Ryzen 7 3800X 8-Core Processor + NVIDIA GeForce RTX 2060 SUPER 11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz + (iGPU) overview # The main take-aways are:\nMost titles downloaded from the internet are h264 encoded, mkv container files. Audio codecs vary and subtitles are complete anarchy. AppleTV (my primary playback client) can play the h264 encoded stream generally without issue. It requires on-the-fly remux from MKV to MP4 and (usually) audio transcoding, but generally uses very little cpu on the plex server. HEVC / h265 can be more space efficient for the same quality, so converting the library to it is worthwhile. However, there are observable limits to how much space can be saved for a given quality level. The main heuristic is thus: if the source file is a high-bitrate, h264 or original iso, then convert to HEVC at about 10mbps (for 1080p). In general, the quality trade off (as well as the energy required to process the transcode) just doesn’t make sense if the file is already below an avg bitrate of 15 mbps. Also, a “how much do i like this movie” factor should be considered for if and how much quality loss is acceptable. Prefer libx265 (cpu / software) based transcodes to GPU-assisted transcodes. GPU is ideal for real-time transcoding. Although it will take much longer, x265 will yield higher quality at smaller space. Consider GPU transcodes for “less important” titles; i.e., movies and series that don’t need “archival quality”. Subtitles are \u0026hellip; annoying if converting from mkv to mp4. The “other transcoding” script seems to handle this, but because mp4 lacks support for lots of subtitle formats, there are still limitations. Donmelton\u0026rsquo;s other_video_transcode tool # Use this script to generate an ffmpeg command that can be executed on another system, if needed.\nhttps://github.com/donmelton/other_video_transcoding\nThis is basically an easier-to-understand wrapper for ffmpeg. Install ruby, then install this with ruby gems. It\u0026rsquo;s helpful to read the script to get a better understanding of all the options and how they map to ffmpeg params.\nIt requires ffmpeg and the mkvtoolnix-cli packages.\nExample with libx265 (software/CPU transcode):\nother-transcode \\ --target 12000 \\ # there are defaults for each resolution, but imo they\u0026#39;re too low --x265 \\ # change this to VAAPI for gpu hw transcode --hevc \\ # no point in not doing HEVC --mp4 \\ # imo it\u0026#39;s widely enough compatible. great for appleTV --add-audio all \\ # without this, the default will strip out all the extraneous audio --add-subtitle eng \\ \u0026lt;/path/to/file.mkv\u0026gt; \\ -n # \u0026#34;dry run\u0026#34; (just prints the ffmpeg commmand)mi Example with iGPU (hardware accelerated transcode):\nother-transcode \\ --vaapi \\ # this is a reliable api for linux + intel w/ iGPU (QuickSync) --target 12000 \\ # VAAPI doesn\u0026#39;t have an easy equivalent of CRF ; this should be tweaked per title --hevc \\ --add-audio all \\ --add-subtitle eng \\ \u0026lt;/path/to/file.mkv\u0026gt; # note: omitted --mp4 here because of subtitle incompatability [Feb 19, 2022] - there is currently a bug in intel-media-driver that results in corrputed video output when using VAAPI (intel iGPU hw accelerated transcode).\n[Feb 20, 2022] - went down the rabbit hole of manually building libva, intel-media-driver (and all intermediate dependencies, yikes!) and ffmpeg with this patch.\nexample ffmpeg command that is generated:\nffmpeg -loglevel error -stats -vaapi_device /dev/dri/renderD128 \\ -i /media/movies/Honeyland\\ \\(2019\\)/Honeyland\\ \\(2019\\)\\ Remux-1080p.mkv \\ -map 0:0 -filter:v yadif,format\\=nv12,hwupload -c:v hevc_vaapi -b:v 9000k \\ -color_primaries:v bt709 -color_trc:v bt709 -colorspace:v bt709 \\ -metadata:s:v title\\= -disposition:v default -map 0:1 -c:​a:0 ac3 \\ -metadata:s:​a:0 title\\= -disposition:​a:0 default -map 0:2 -c:​a:1 aac \\ -metadata:s:​a:1 title\\= -disposition:​a:1 0 -sn \\ -metadata:g title\\= -default_mode passthrough \\ Honeyland\\ \\(2019\\)\\ Remux-1080p.mkv general ffmpeg param structure:\nffmpeg -i \u0026lt;input file\u0026gt; \\ -stats \\ -\u0026lt;extra params\u0026gt; \\ \u0026lt;output_file_name\u0026gt; (likely rare), but remux + transcode BR disk .iso rip:\nhttps://unixsheikh.com/tutorials/remuxing-iso-dvd-or-bluray-using-cat-and-ffmpeg-on-linux.html\nessentially, the process is 1) mount the iso, find the correct m2ts file, combine them if necessary, then ffmpeg to package it up into .mkv and transcode if desired. Here’s an example of processing Apollo 11 blu-ray full disk:\nffmpeg -i br/BDMV/STREAM/00006.m2ts -stats -map 0:0 -pix_fmt yuv420p10le -c:v libx265 -x265-params profile=main10:crf=20 -tag:v hvc1 -map 0:​a \u0026#39;Apollo 11 (2019) Remux-2160p.mp4’ using CRF for 4k main10:\nnohup ffmpeg -loglevel error -stats -i ../Jojo\\ Rabbit\\ \\(2019\\)\\ Remux-2160p.mkv \\ -map 0:0 \\ -c:v libx265 -pix_fmt:v yuv420p10le -crf 17 -preset slower \\ -color_primaries:v bt2020 -color_trc:v smpte2084 -colorspace:v bt2020nc \\ -metadata:s:v title\\= -disposition:v default \\ -map 0:1 -c:​a:0 ac3 -metadata:s:​a:0 title\\= -disposition:​a:0 default \\ -sn -metadata:g title\\= -default_mode passthrough \\ Jojo\\ Rabbit\\ \\(2019\\)\\ Remux-2160p.mkv \u0026amp; Quality checking the transcoded output is essential. further reading # https://codecalamity.com/encoding-settings-for-hdr-4k-videos-using-10-bit-x265/ - tl;dr CRF 20 preset slow\nApple TV 4k hevc - Use the -tag:v hvc1 parameter in your FFmpeg command make your HEVC file work on Apple devices.\nhttps://codecalamity.com/stop-re-encoding-videos/\nhttps://slhck.info/video/2017/02/24/crf-guide.html\nhttps://netflixtechblog.com/per-title-encode-optimization-7e99442b62a2 - very interesting. Big takeaways are:\nevery netflix title is considered to have its own optimal bitrate + quality level for each resolution Lower resolution + higher precision can result in greater quality than higher res + lower precision, but mostly at the low side of the quality curve. netflix stores multiple encodes per resolution, per title. Storage tradeoff gives bandwidth + user experience wins Using VAAPI\u0026rsquo;s hardware accelerated video encoding on Linux with Intel\u0026rsquo;s hardware on FFmpeg and libav\nstill TODO / open questions # test performance / effect of performing conversion over NFS for the ryzen + Nvidia system, the source file is first copied to the local machine when a 3rd system is used for transcode directly via NFS, CPU on the NFS server rises to about 25% usage! audio codecs probably can find a good preset for “basic 5.1” ","date":"11 June 2022","permalink":"/posts/video_transcode/","section":"Posts","summary":"meta # These are some notes gathered on my exploration of transcoding videos for more reliable playback.","title":"video transcoding"}]